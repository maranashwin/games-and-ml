{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c09cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e60807f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.emma.', '.olivia.', '.ava.', '.isabella.', '.sophia.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('names.txt', 'r', encoding='utf-8')\n",
    "names = f.read().split(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "names = [\".\" + name + \".\" for name in names]\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3c5939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHARS = sorted(list(set(\"\".join(names))))\n",
    "\"\".join(CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ee7016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.hello.world.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(CHARS)}\n",
    "itos = {i: ch for i, ch in enumerate(CHARS)}\n",
    "\n",
    "text = \".hello.world.\"\n",
    "\"\".join([itos[i] for i in [stoi[ch] for ch in text]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5364da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for name in names:\n",
    "            for ch1, ch2 in zip(name, name[1:]):\n",
    "                self.x.append(stoi[ch1])\n",
    "                self.y.append(stoi[ch2])\n",
    "        self.x = torch.tensor(self.x)\n",
    "        self.y = torch.tensor(self.y)\n",
    "        self.x = F.one_hot(self.x, num_classes=len(CHARS)).float()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab594244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(len(CHARS), len(CHARS))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.lin(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37aaaf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logP_loss(model, probs, y, alpha=1e-3):\n",
    "    logP = -probs[torch.arange(len(y)), y].log().mean()\n",
    "    reg_loss = alpha * sum([(p**2).sum() for p in model.parameters()])\n",
    "    return logP + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7a2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs=5, lr=1e-1, alpha=1e-3):\n",
    "    model.to(\"cuda\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    prev_loss = None\n",
    "    for epoch in range(epochs):\n",
    "        old_model_state = model.state_dict()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_total = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "            probs = model(x)\n",
    "            loss = logP_loss(model, probs, y, alpha)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*y.size(0)\n",
    "            train_total += y.size(0)\n",
    "        train_loss /= train_total\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "                probs = model(x)\n",
    "                loss = logP_loss(model, probs, y, alpha)\n",
    "                val_loss += loss.item()*y.size(0)\n",
    "                val_total += y.size(0)\n",
    "            val_loss /= val_total\n",
    "                \n",
    "        print(f\"Epoch {epoch+1}: Learning Rate={lr}, Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "        if prev_loss is not None and train_loss + val_loss > prev_loss:\n",
    "            model.load_state_dict(old_model_state)\n",
    "            lr /= 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            prev_loss = train_loss + val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be2358f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makemore(model):\n",
    "    letters = [\".\"]\n",
    "    \n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            x = F.one_hot(torch.tensor(stoi[letters[-1]]).unsqueeze(0), num_classes=len(CHARS)).float().to(\"cuda\")\n",
    "            probs = model(x)\n",
    "            char = itos[torch.multinomial(probs, num_samples=1, replacement=True).item()]\n",
    "            letters.append(char)\n",
    "            if char == \".\":\n",
    "                break\n",
    "    return \"\".join(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "912523d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "split = int(len(names)*0.9)\n",
    "train_data = NameDataset(names[:split])\n",
    "val_data = NameDataset(names[split:])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=1024)\n",
    "\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea449587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Learning Rate=0.01, Train Loss=2.6970, Val Loss=2.5395\n",
      "Epoch 2: Learning Rate=0.01, Train Loss=2.5253, Val Loss=2.5193\n",
      "Epoch 3: Learning Rate=0.01, Train Loss=2.5142, Val Loss=2.5137\n",
      "Epoch 4: Learning Rate=0.01, Train Loss=2.5101, Val Loss=2.5108\n",
      "Epoch 5: Learning Rate=0.01, Train Loss=2.5084, Val Loss=2.5098\n",
      "Epoch 6: Learning Rate=0.01, Train Loss=2.5077, Val Loss=2.5094\n",
      "Epoch 7: Learning Rate=0.01, Train Loss=2.5073, Val Loss=2.5092\n",
      "Epoch 8: Learning Rate=0.01, Train Loss=2.5071, Val Loss=2.5089\n",
      "Epoch 9: Learning Rate=0.01, Train Loss=2.5070, Val Loss=2.5087\n",
      "Epoch 10: Learning Rate=0.01, Train Loss=2.5069, Val Loss=2.5086\n"
     ]
    }
   ],
   "source": [
    "model = SimpleGram()\n",
    "train(model, train_loader, val_loader, epochs=10, lr=1e-2, alpha=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ca607d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0008, 0.0468, 0.0180, 0.0114, 0.0351, 0.0506, 0.0236, 0.0177, 0.1305,\n",
       "        0.0333, 0.0165, 0.0285, 0.0449, 0.0331, 0.1546, 0.0252, 0.0147, 0.0241,\n",
       "        0.0350, 0.0457, 0.0277, 0.0115, 0.0188, 0.0177, 0.0447, 0.0672, 0.0225],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0][0].softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b60c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ka.\n",
      ".tiry.\n",
      ".ainionedeylyaomottrmilalana.\n",
      ".hifenahobliahts.\n",
      ".n.\n",
      ".treeyaze.\n",
      ".mayahanari.\n",
      ".aolanay.\n",
      ".krarje.\n",
      ".lialaer.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(makemore(model)[1:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
