{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af54cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f1c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85275644",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273557d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKN = \"<\"\n",
    "END_TOKN = \">\"\n",
    "MISS_TOKN = \"*\"\n",
    "PAD_TOKN = \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d984745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204570, 22730)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read(file):\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    data = f.read().splitlines()\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "train_data = read('train_data.txt')\n",
    "val_data = read('val_data.txt')\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18b7d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_WORD_LEN = max([len(w) for w in train_data]) + 1 # extra padding\n",
    "BLOCK_SIZE = MAX_WORD_LEN + 2\n",
    "MAX_WORD_LEN, BLOCK_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a94d3569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#*<>abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHARS = sorted(list(set(\"\".join(train_data))) + [START_TOKN, END_TOKN, MISS_TOKN, PAD_TOKN])\n",
    "\"\".join(CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef17ec3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<hello*world#>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(CHARS)}\n",
    "itos = {i: ch for i, ch in enumerate(CHARS)}\n",
    "\n",
    "text = START_TOKN + \"hello\" + MISS_TOKN + \"world\" + PAD_TOKN + END_TOKN\n",
    "tokns = [stoi[ch] for ch in text]\n",
    "\"\".join([itos[i] for i in tokns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58c3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, emb=32, heads=4):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.key = nn.Linear(emb, emb, bias=False)\n",
    "        self.qry = nn.Linear(emb, emb)\n",
    "        self.val = nn.Linear(emb, emb)\n",
    "        self.proj = nn.Linear(emb, emb)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, z, attn_mask=None, rel_pos_emb=None):\n",
    "        B, Tx, C = x.shape                                                # (B, Tx, C)\n",
    "        Tz = z.shape[1]                                                   # (B, Tz, C)\n",
    "        H = self.heads\n",
    "        q = self.qry(x).view(B, Tx, H, C // H).transpose(1, 2)            # (B, H, Tx, C//H)\n",
    "        k = self.key(z).view(B, Tz, H, C // H).transpose(1, 2)            # (B, H, Tz, C//H)\n",
    "        v = self.val(z).view(B, Tz, H, C // H).transpose(1, 2)            # (B, H, Tz, C//H)\n",
    "        att = (q @ k.transpose(-1, -2)) * (1.0 / np.sqrt(k.size(-1)))     # (B, H, Tx, Tz)\n",
    "        if rel_pos_emb is not None:\n",
    "            att += rel_pos_emb                                            # (B, H, Tx, Tz)\n",
    "        if attn_mask is not None:\n",
    "            att = att.masked_fill(attn_mask == 1, -1e9)                   # (B, H, Tx, Tz)\n",
    "        att = self.dropout(F.softmax(att, dim=-1))                        # (B, H, Tx, Tz)\n",
    "        y = (att @ v).transpose(1, 2)                                     # (B, Tx, H, C//H)\n",
    "        y = y.contiguous().view(B, Tx, C)                                 # (B, Tx, C)\n",
    "        y = self.dropout(self.proj(y))                                    # (B, Tx, C)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e359806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb=32, mlp_emb=128):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(emb, mlp_emb)\n",
    "        self.proj = nn.Linear(mlp_emb, emb)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):                                                    # (B, T, C)\n",
    "        x = self.lin(x)                                                      # (B, T, M)\n",
    "        x = F.gelu(x)                                                        # (B, T, M)\n",
    "        y = self.proj(x)                                                     # (B, T, C)\n",
    "        y = self.dropout(y)                                                  # (B, T, C)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9957c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, heads=4, block_size=BLOCK_SIZE):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.bias = nn.Embedding(2*block_size+1, heads)\n",
    "        \n",
    "    def forward(self, Tx, Tz):\n",
    "        pos = torch.arange(Tx, device=DEVICE)\n",
    "        rel_pos = pos[:, None] - pos[None, :]                                # (Tx, Tz)\n",
    "        rel_pos = rel_pos + self.block_size                                  # (Tx, Tz)\n",
    "        bias = self.bias(rel_pos)                                            # (Tx, Tz, H)\n",
    "        return bias.permute(2, 0, 1)                                         # (H, Tx, Tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b67b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, emb=32, heads=4, mlp_emb=128, x_attn=False, rel_pos=False):\n",
    "        super().__init__()\n",
    "        self.self_norm = nn.LayerNorm(emb)\n",
    "        self.self_attn = Attention(emb, heads)\n",
    "        self.self_res_scale = nn.Parameter(torch.tensor(0.1))\n",
    "        self.feed_fwd_norm = nn.LayerNorm(emb)\n",
    "        self.feed_fwd = FeedForward(emb, mlp_emb)\n",
    "        self.feed_fwd_res_scale = nn.Parameter(torch.tensor(0.1))\n",
    "        self.x_attn = x_attn\n",
    "        if self.x_attn:\n",
    "            self.cross_norm = nn.LayerNorm(emb)\n",
    "            self.cross_attn = Attention(emb, heads)\n",
    "            self.cross_res_scale = nn.Parameter(torch.tensor(0.1))\n",
    "        self.rel_pos = rel_pos\n",
    "        if self.rel_pos:\n",
    "            self.rel_pos_bias = RelativePositionBias(heads) \n",
    "            \n",
    "    def forward(self, x, z=None, self_mask=None, x_mask=None):\n",
    "        nx = self.self_norm(x)                                              # (B, Tx, C)\n",
    "        rel_pos_emb = None\n",
    "        if self.rel_pos:\n",
    "            rel_pos_emb = self.rel_pos_bias(nx.size(1), nx.size(1))         # (H, Tx, Tx)\n",
    "            rel_pos_emb = rel_pos_emb.unsqueeze(0)                          # (1, H, Tx, Tx)\n",
    "        self_attn = self.self_attn(nx, nx, self_mask, rel_pos_emb)          # (B, Tx, C)\n",
    "        attn = x + self.self_res_scale * self_attn                          # (B, Tx, C)\n",
    "        if self.x_attn:\n",
    "            nx = self.cross_norm(attn)                                      # (B, Tx, C)\n",
    "            x_attn = self.cross_attn(nx, z, x_mask)                         # (B, Tx, C)\n",
    "            attn = attn + self.cross_res_scale * x_attn                     # (B, Tx, C)\n",
    "        nx = self.feed_fwd_norm(attn)                                       # (B, Tx, C)\n",
    "        ff = attn + self.feed_fwd_res_scale * self.feed_fwd(nx)             # (B, Tx, C)\n",
    "        return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e47892fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb=32, heads=4, mlp_emb=128, decoder_layers=4, encoder_layers=2):\n",
    "        super().__init__()\n",
    "        self.char_embed = nn.Embedding(len(CHARS), emb)\n",
    "        self.guess_embed = nn.Linear(len(CHARS), emb)\n",
    "        \n",
    "        self.guess_blocks = nn.Sequential(*[Block(emb, heads, mlp_emb) for _ in range(encoder_layers)])\n",
    "        self.decoder_blocks = nn.ModuleList([Block(emb, heads, mlp_emb, True, True) for _ in range(decoder_layers)])\n",
    "        \n",
    "        self.decoder_norm = nn.LayerNorm(emb)\n",
    "        self.decoder = nn.Linear(emb, len(CHARS), bias=False)\n",
    "        self.decoder.weight = self.char_embed.weight\n",
    "\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        B, T = x.shape                                                        # (B, T)\n",
    "        char_emb = self.char_embed(x)                                         # (B, T, C)\n",
    "        g_emb = self.guess_embed(g).unsqueeze(1).expand(-1, T, -1)            # (B, T, C)\n",
    "\n",
    "        pos_mask = (x == stoi[PAD_TOKN])                                      # (B, T)\n",
    "        \n",
    "        attn_mask = pos_mask.unsqueeze(1) | pos_mask.unsqueeze(2)             # (B, T, T)\n",
    "        attn_mask = attn_mask.unsqueeze(1)                                    # (B, 1, T, T)\n",
    "        \n",
    "        qry_mask = pos_mask.unsqueeze(1).unsqueeze(2)                         # (B, 1, 1, T)\n",
    "        qry_mask = qry_mask.expand(-1, -1, T, -1)                             # (B, 1, T, T)\n",
    "        \n",
    "        g_z = self.guess_blocks(g_emb)                                        # (B, T, C)\n",
    "        out = char_emb                                                        # (B, T, C)\n",
    "        for block in self.decoder_blocks:\n",
    "            out = block(out, g_z, self_mask=attn_mask, x_mask=qry_mask)       # (B, T, C)  \n",
    "        \n",
    "        out = self.decoder_norm(out)                                          # (B, T, C)\n",
    "        out = self.decoder(out)                                               # (B, T, V)\n",
    "        out = out.transpose(-1, -2)                                           # (B, V, T)\n",
    "        decoder_mask = (x == stoi[MISS_TOKN])                                 # (B, T)\n",
    "        num_masked = decoder_mask.sum(dim=1, keepdim=True)                    # (B, 1)\n",
    "        masked_out = out.masked_fill(~decoder_mask.unsqueeze(1), 0.0)         # (B, V, T)\n",
    "        logits = masked_out.sum(dim=-1) / num_masked                          # (B, V)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f5e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, word, guessed):\n",
    "    model.eval()\n",
    "\n",
    "    word =  word.replace(\".\", MISS_TOKN)\n",
    "    x_ch = (START_TOKN + word + END_TOKN + PAD_TOKN*BLOCK_SIZE)[:BLOCK_SIZE]\n",
    "    x = torch.tensor([stoi[ch] for ch in x_ch], device=DEVICE)\n",
    "    g = torch.zeros(len(CHARS), device=DEVICE)\n",
    "    g_tkn = [stoi[ch] for ch in guessed + list(word) if ch != MISS_TOKN]\n",
    "    g[g_tkn] = 1.0\n",
    "    logits = model(x.unsqueeze(0), g.unsqueeze(0))\n",
    "    logits = logits.squeeze(0)\n",
    "\n",
    "    preds = {itos[i]: logits[i].item() for i in range(len(logits))}\n",
    "    return sorted(preds.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f55458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11601760"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(emb=256, heads=8, mlp_emb=1024, decoder_layers=8, encoder_layers=4)\n",
    "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "model.to(\"cuda\")\n",
    "\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e37312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanInit(Dataset):\n",
    "    def __init__(self, words, max_length=MAX_WORD_LEN):\n",
    "        self.words = words\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        \n",
    "        x_tkn = (START_TOKN + word + END_TOKN + PAD_TOKN * BLOCK_SIZE)[:BLOCK_SIZE]\n",
    "        x = torch.tensor([stoi[c] for c in x_tkn], device=DEVICE)\n",
    "\n",
    "        g = torch.zeros(len(CHARS), device=DEVICE)\n",
    "        y = torch.zeros(len(CHARS), device=DEVICE)\n",
    "        y[[stoi[c] for c in list(set(word))]] = 1.0\n",
    "\n",
    "        return (x, g, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1c6ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(ds):\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, sampler=RandomSampler(ds, replacement=True, num_samples=BATCH_SIZE))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c301f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, words, g, y, max_tries=6):\n",
    "        self.words = words\n",
    "        self.g = g\n",
    "        self.y = y\n",
    "        x_mask = (self.words != stoi[START_TOKN]) & (self.words != stoi[END_TOKN]) & (self.words != stoi[PAD_TOKN])\n",
    "        self.x = self.words.masked_fill(x_mask, stoi[MISS_TOKN])\n",
    "        self.max_tries = max_tries\n",
    "        self.fails = torch.zeros(self.y.size(0), dtype=torch.int, device=DEVICE)\n",
    "        \n",
    "    def get_state(self):\n",
    "        return (self.x, self.g)\n",
    "    \n",
    "    def is_done(self):\n",
    "        return len(self.g) == 0\n",
    "        \n",
    "    def update(self, guesses):\n",
    "        B, V = self.y.shape\n",
    "        T = self.x.size(1)\n",
    "\n",
    "        correct_guesses = self.y[torch.arange(B, device=DEVICE), guesses].bool()\n",
    "        new_guesses = F.one_hot(guesses, V).float()\n",
    "\n",
    "        self.g = torch.maximum(self.g, new_guesses)\n",
    "        self.y = torch.clamp(self.y - new_guesses * correct_guesses.unsqueeze(1).float(), min=0.0)\n",
    "\n",
    "        expanded_guesses = guesses.unsqueeze(1).expand(-1, T)\n",
    "        mask = self.words == expanded_guesses\n",
    "        self.x = torch.where(mask, expanded_guesses, self.x)\n",
    "\n",
    "        self.fails[~correct_guesses] += 1\n",
    "        won = (self.y == 0).all(dim=1)\n",
    "        lost = (self.fails >= self.max_tries)\n",
    "\n",
    "        keep = ~(won | lost)\n",
    "        self.words = self.words[keep]\n",
    "        self.x = self.x[keep]\n",
    "        self.g = self.g[keep]\n",
    "        self.y = self.y[keep]\n",
    "        self.fails = self.fails[keep]\n",
    "\n",
    "        return won.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf791f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, x, g):\n",
    "    model.eval()\n",
    "    with torch.autocast(device_type=DEVICE, dtype=torch.bfloat16):\n",
    "        logits = model(x, g)\n",
    "    masked_logits = logits.masked_fill(g.bool(), -1e9)\n",
    "    guesses = masked_logits.argmax(dim=1)\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a61d31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    w_init, g_init, y_init = next(iter(loader))\n",
    "    env = GameEnv(w_init, g_init, y_init)\n",
    "    total_wins = 0\n",
    "\n",
    "    while not env.is_done():\n",
    "        x, g = env.get_state()\n",
    "        guesses = predict(model, x, g)\n",
    "        total_wins += env.update(guesses)\n",
    "\n",
    "    return total_wins/w_init.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e5747c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_rates(model, data, batches=10**3):\n",
    "    wins = []\n",
    "    loader = get_loader(HangmanInit(data))\n",
    "    \n",
    "    for _ in range(batches):\n",
    "        wins.append(validate(model, loader))\n",
    "        if (_+1) % max(1, (batches//10)) == 0 or (_+1) == batches:\n",
    "            print(f\"Step {_+1}: Win Rate={sum(wins)/len(wins)*100:.2f}%\")\n",
    "    \n",
    "    num_games = BATCH_SIZE*len(wins)\n",
    "    num_wins = sum(wins)*BATCH_SIZE\n",
    "    p = num_wins/num_games\n",
    "    var = p*(1 - p)/num_games\n",
    "    std = np.sqrt(var)\n",
    "    print()\n",
    "    print(f\"Won {int(num_wins)} out of {int(num_games)} games; E[p]={p:.4f}, Var(p)={var:.4f}, std(p)={std:.4f}\")\n",
    "    print(f\"Win Rate between {(p - std)*100:.2f}% and {(p + std)*100:.2f}% with 65% probability\")\n",
    "    print(f\"Win Rate between {(p - 2*std)*100:.2f}% and {(p + 2*std)*100:.2f}% with 95% probability\")\n",
    "    print(f\"Win Rate between {(p - 3*std)*100:.2f}% and {(p + 3*std)*100:.2f}% with 99.7% probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf83a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset...\n",
      "Step 100: Win Rate=71.19%\n",
      "Step 200: Win Rate=71.80%\n",
      "Step 300: Win Rate=71.18%\n",
      "Step 400: Win Rate=71.21%\n",
      "Step 500: Win Rate=71.24%\n",
      "Step 600: Win Rate=71.18%\n",
      "Step 700: Win Rate=71.00%\n",
      "Step 800: Win Rate=71.05%\n",
      "Step 900: Win Rate=71.11%\n",
      "Step 1000: Win Rate=71.04%\n",
      "\n",
      "Won 45464 out of 64000 games; E[p]=0.7104, Var(p)=0.0000, std(p)=0.0018\n",
      "Win Rate between 70.86% and 71.22% with 65% probability\n",
      "Win Rate between 70.68% and 71.40% with 95% probability\n",
      "Win Rate between 70.50% and 71.58% with 99.7% probability\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset...\")\n",
    "get_win_rates(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aa44bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset...\n",
      "Step 100: Win Rate=68.44%\n",
      "Step 200: Win Rate=66.91%\n",
      "Step 300: Win Rate=66.65%\n",
      "Step 400: Win Rate=66.86%\n",
      "Step 500: Win Rate=66.91%\n",
      "Step 600: Win Rate=66.88%\n",
      "Step 700: Win Rate=66.98%\n",
      "Step 800: Win Rate=66.98%\n",
      "Step 900: Win Rate=66.98%\n",
      "Step 1000: Win Rate=66.92%\n",
      "\n",
      "Won 42826 out of 64000 games; E[p]=0.6692, Var(p)=0.0000, std(p)=0.0019\n",
      "Win Rate between 66.73% and 67.10% with 65% probability\n",
      "Win Rate between 66.54% and 67.29% with 95% probability\n",
      "Win Rate between 66.36% and 67.47% with 99.7% probability\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset...\")\n",
    "get_win_rates(model, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9c328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
